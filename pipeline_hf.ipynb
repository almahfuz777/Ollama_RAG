{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"phy_book_ch2.pdf\"  # Path to your PDF file\n",
    "if pdf_path:\n",
    "    loader = PyPDFLoader(file_path=pdf_path)\n",
    "    data = loader.load()\n",
    "else:\n",
    "    print(\"PDF not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26  Physics \n",
      "Chapter Two  \n",
      "MOTION \n",
      " \n",
      " \n",
      "[The object, that we see around us either are stationery or in motion. What do we \n",
      "actually understand by the words ``rest’’ and ``motion’’. We need different quantities regarding motion to express the characteristics of motion of a moving object. In this chapter we will discuss different quantities regarding motion, their dimensions, units, the \n",
      "relations among them etc.] \n",
      "By the end of this chapter we will be able to -  \n",
      "1. Explain the rest and motion  \n",
      "2. Find out the difference among different types of motion.  \n",
      "3. Explain the scalar and vector quantities  \n",
      "4. Analyze the relation among the quantities regarding motion  5. Explain the motion of freely falling bodies  \n",
      "6. Analyze the relations among the quantities regarding motion with the help of graph \n",
      "7. Realize the effect of motion in our life    \n"
     ]
    }
   ],
   "source": [
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "cleaned_data = []\n",
    "for doc in data:\n",
    "    cleaned_data.append(Document(page_content=clean_text(doc.page_content), metadata=doc.metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    r'∴.*',        # Lines starting with '∴' (therefore)\n",
    "    r'=[^=]*',     # Equal signs\n",
    "    r'\\b[^\\s]+/[^\\s]+\\b',  # Fractions like 'distance/time'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_math_expressions(text):\n",
    "    math_expressions = []\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, text)\n",
    "        math_expressions.extend(matches)\n",
    "    return math_expressions\n",
    "\n",
    "for doc in cleaned_data:\n",
    "    math_exprs = extract_math_expressions(doc.page_content)\n",
    "    if isinstance(math_exprs, list):\n",
    "        math_exprs = \"; \".join(math_exprs)\n",
    "    doc.metadata['math_expressions'] = math_exprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = []\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        table = page.extract_table()\n",
    "        if table:\n",
    "            tables.append(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_dfs = [pd.DataFrame(table[1:], columns=table[0]) for table in tables]\n",
    "table_texts = [df.to_string(index=False) for df in table_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_documents = [Document(page_content=text, metadata={'source': 'table'}) for text in table_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = cleaned_data + table_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_complex_metadata(metadata):\n",
    "    filtered_metadata = {}\n",
    "    for key, value in metadata.items():\n",
    "        if isinstance(value, (str, int, float, bool)):\n",
    "            filtered_metadata[key] = value\n",
    "        else:\n",
    "            filtered_metadata[key] = str(value)\n",
    "    return filtered_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_metadatas = [filter_complex_metadata(doc.metadata) for doc in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_contents = [chunk.page_content for chunk in chunks]\n",
    "metadatas = [chunk.metadata for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_16576\\3802521539.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "embedded_texts = embedding.embed_documents(page_contents)\n",
    "chunked_documents = [Document(page_content=content, metadata=meta) for content, meta in zip(page_contents, metadatas)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded Documents stored in ChromaDB successfully!\n"
     ]
    }
   ],
   "source": [
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunked_documents,\n",
    "    embedding=embedding,\n",
    "    collection_name=\"local-rag\",\n",
    "    persist_directory=\"./db_HF\"  # Path to save the ChromaDB data\n",
    ")\n",
    "\n",
    "print(\"Embedded Documents stored in ChromaDB successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
