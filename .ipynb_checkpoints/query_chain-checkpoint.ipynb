{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydantic import BaseModel  # to handle pydantic error while importing OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma.vectorstores import Chroma\n",
    "# from langchain_community.llms import Ollamaabs\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the persisted 'nomic-embed-text' db\n",
    "# embedding = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True)\n",
    "# persist_directory = \"./db/db_nomic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\almah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\almah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the persisted 'all-MiniLM-L6-v2' db\n",
    "embedding = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "persist_directory = \"./db/db_minilm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database loaded\n"
     ]
    }
   ],
   "source": [
    "# Load the persisted ChromaDB vector store\n",
    "\n",
    "vector_database = Chroma(\n",
    "    collection_name=\"local-rag\",\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")\n",
    "print('database loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\almah\\AppData\\Local\\Temp\\ipykernel_19720\\3043566614.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3.2\")\n"
     ]
    }
   ],
   "source": [
    "# llm = Ollama(model=\"llama3.1\")\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "# llm = Ollama(model=\"mistral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:lime;\"> Creates 5 different versions of the original question. and then retrieves 3 relevant documents from the database based on those questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"\n",
    "    You are an AI language model specialized in physics. Your task is to reformulate the following question into five different versions to retrieve the most relevant physics documents.\n",
    "    Original question: {question}\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_database.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    llm,\n",
    "    prompt = QUERY_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt\n",
    "template = \"\"\"You are a helpful assistant trained to answer physics questions based on the provided context.\n",
    "Use only the context below to answer the following question as clearly as possible:\n",
    "{context}\n",
    "Question: \n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print relevant chunks\n",
    "# def print_relevant_chunks(query, count):\n",
    "#     chunks = vector_database.similarity_search_with_score(query, k=count)\n",
    "#     print(\"relevant chunks:\")\n",
    "#     for chunk in chunks:\n",
    "#         print(chunk)\n",
    "#     print(\"--------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_relevant_chunks(query, count):\n",
    "    retrieved_docs = retriever.get_relevant_documents(##)\n",
    "    print(\"relevant docs:\")\n",
    "    for doc in retrieved_docs:\n",
    "        print(doc)\n",
    "    print(\"--------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_answer(answer):\n",
    "    # Ensure the answer is focused on the physics topic\n",
    "    # Optionally trim any irrelevant parts or hallucinated information\n",
    "    processed_ans =  answer.split(\"Answer:\")[-1].strip()\n",
    "    return processed_ans if processed_ans else \"No relevant info found in the context...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAns(query):\n",
    "    answer = chain.invoke(query)\n",
    "    final_ans = post_process_answer(answer)\n",
    "    output = \"query: \"+query+\"\\n\"+ \"answer:\\n\"+final_ans\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultiQueryRetriever' object has no attribute 'vectorstore'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mprint_relevant_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(getAns(query))\n",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m, in \u001b[0;36mprint_relevant_chunks\u001b[1;34m(query, count)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_relevant_chunks\u001b[39m(query, count):\n\u001b[1;32m----> 2\u001b[0m     retrieved_docs \u001b[38;5;241m=\u001b[39m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241m.\u001b[39msearch(query\u001b[38;5;241m=\u001b[39mquery, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelevant docs:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m retrieved_docs:\n",
      "File \u001b[1;32mc:\\Users\\almah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\main.py:828\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 828\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MultiQueryRetriever' object has no attribute 'vectorstore'"
     ]
    }
   ],
   "source": [
    "query = input()\n",
    "print_relevant_chunks(query, 5)\n",
    "print(getAns(query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
